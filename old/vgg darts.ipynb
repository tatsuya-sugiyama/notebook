{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"vgg darts.ipynb","provenance":[],"collapsed_sections":["SUQfCKp2cnFq","OUQoaMlybaLE","TtrTvApwmZ-g","gii9_gAMkEyV","pmMw2Fhi2gdl","EuByAf2cLRoS"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"eEoEfZteii7G"},"source":["import os\n","import sys\n","import time\n","import glob\n","import json\n","import pytz\n","import numpy as np\n","import logging\n","import argparse\n","import datetime\n","import graphviz\n","import networkx as nx\n","import matplotlib.pyplot as plt\n","\n","import torch #基本モジュール\n","from torch.autograd import Variable #自動微分用\n","import torch.nn as nn #ネットワーク構築用\n","import torch.optim as optim #最適化関数\n","import torch.nn.functional as F #ネットワーク用の様々な関数\n","import torch.utils.data #データセット読み込み関連\n","import torchvision #画像関連\n","from torchvision import datasets, models, transforms #画像用データセット諸々\n","import torch.backends.cudnn as cudnn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YfnROzhgDEGp","executionInfo":{"status":"ok","timestamp":1600928553300,"user_tz":-540,"elapsed":29127,"user":{"displayName":"Yy Yy","photoUrl":"","userId":"02437050620110452344"}},"outputId":"b7a18e80-5cf0-4c34-d82b-e9d51577b0b5","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LWsydO_m0MtN"},"source":["# utils"]},{"cell_type":"markdown","metadata":{"id":"fM8zsEyXcWuk"},"source":["## other"]},{"cell_type":"code","metadata":{"id":"ZCJioEPw2esA"},"source":["def argspace(**kwds):\n","  def deco(f):\n","    def inner(**ikwds):\n","      kwds.update(ikwds)\n","      return f(argparse.Namespace(**kwds))\n","    return inner\n","  return deco"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6ml84YYEzZ_4"},"source":["def set_seed(seed):\n","  np.random.seed(seed)\n","  torch.manual_seed(seed)\n","  torch.cuda.manual_seed(seed)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vPVS7mOfYboW"},"source":["class Singleton(object):\n","  def __new__(cls, *args, **kargs):\n","    if not hasattr(cls, \"_instance\"):\n","      cls._instance = super(Singleton, cls).__new__(cls)\n","    return cls._instance"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NiwPvH9hBAqs"},"source":["class Experiment(Singleton):\n","  def __init__(self):\n","    if not hasattr(self, 'func'):\n","      self.func = {}\n","  \n","  def _store(self, key, f, order):\n","    self.func.setdefault(key, {})\n","    self.func[key].update({f.__name__:(order, f)})\n","\n","  def __call__(self, key):\n","    def f(*args, **kwds):\n","      funcs = sorted(self.func[key].values(), key=lambda x: x[0])\n","      return [g(*args, **kwds) for _, g in funcs]\n","    return f\n","\n","  def event(*key, order=0):\n","    def d(f):\n","      for k in key:\n","        Experiment()._store(k, f, order)\n","      def inner(*args, **kwds):\n","        return f(*args, **kwds)\n","      return inner\n","    return d\n","  \n","  def reset():\n","    Experiment().func = {}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DPWZJIgIcfAg"},"source":["## metrics"]},{"cell_type":"code","metadata":{"id":"CJ28A29Kjm_4"},"source":["def accuracy(output, target, topk=(1,)):\n","  maxk = max(topk)\n","  batch_size = target.size(0)\n","\n","  _, pred = output.topk(maxk, 1, True, True)\n","  pred = pred.t()\n","  correct = pred.eq(target.view(1, -1).expand_as(pred))\n","\n","  res = []\n","  for k in topk:\n","    correct_k = correct[:k].view(-1).float().sum(0)\n","    res.append(correct_k.mul_(100.0/batch_size))\n","  return res"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZZUgjfdokzq8"},"source":["class AvgrageMeter(object):\n","\n","  def __init__(self):\n","    self.reset()\n","\n","  def reset(self):\n","    self.avg = 0\n","    self.sum = 0\n","    self.cnt = 0\n","\n","  def update(self, val, n=1):\n","    self.sum += val * n\n","    self.cnt += n\n","    self.avg = self.sum / self.cnt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5HIFrNph_P8t"},"source":["class Metrics():\n","  def __init__(self, model, dataloader):\n","    self._run(model, dataloader)\n","    self.epsilon = 1e-7\n","  \n","  @torch.no_grad()\n","  def _run(self, model, dataloader):\n","    data, target = next(iter(dataloader))\n","    device =  next(model.parameters()).device\n","    print(device)\n","    num = target.max().long() + 1\n","    cm = torch.zeros(num, num).to(device)\n","\n","    for i, (data, target) in enumerate(dataloader):\n","      data = data.to(device)\n","      target = target.to(device)\n","      outputs = model(data)\n","      _, preds = torch.max(outputs, 1)\n","      for t, p in zip(target.view(-1), preds.view(-1)):\n","        cm[t.long(), p.long()] += 1\n","\n","    self.matrix = cm\n","    self.dim = num\n","    self.sum0 = self.matrix.sum(0)\n","    self.sum1 = self.matrix.sum(1)\n","    self.sum = self.matrix.sum()\n","\n","  def confusion_matrix(self):\n","    return self.matrix\n","\n","  def TP(self, index):\n","    return self.matrix[index][index]\n","\n","  def FN(self, index):\n","    return self.sum1[index] - self.TP(index)\n","\n","  def FP(self, index):\n","    return self.sum0[index] - self.TP(index)\n","\n","  def TN(self, index):\n","    return self.sum - self.TP(index) - self.FN(index) - self.FP(index)\n","\n","  def _sum(self, F):\n","    return sum(F(i) for i in range(self.dim))\n","    \n","  def _micro(self, F, G):\n","    return self._sum(F) / (self._sum(F) + self._sum(G) + self.epsilon)\n","\n","  def _macro(self, F, G):\n","    return sum(F(i) / (F(i) + G(i) + self.epsilon) for i in range(self.dim)) / self.dim\n","\n","  def _switch(self, F, G, micro):\n","    return (self._micro(F, G) if micro else self._macro(F, G))\n","\n","  def accuracy(self, micro=True):\n","    return (self._sum(self.TP) / self.sum if micro else \n","            (sum(self.TP(i) / self.sum1 for i in range(self.dim)) / self.dim).mean())\n","\n","  def precision(self, micro=True):\n","    return self._switch(self.TP, self.FP, micro)\n","\n","  def recall(self, micro=True):\n","    return self._switch(self.TP, self.FN, micro)\n","\n","  def specificity(self, micro=True):\n","    return self._switch(self.TN, self.FP, micro)\n","\n","  def f_measure(self, micro=True):\n","    p, r = self.precision(micro), self.recall(micro)\n","    return 2 * p * r / (p + r + self.epsilon)\n","\n","  def print(self):\n","    print(self.confusion_matrix())\n","    print(\"accuracy \", self.accuracy(), self.accuracy(micro=False))\n","    print(\"precision \", self.precision(), self.precision(micro=False))\n","    print(\"recall \", self.recall(), self.recall(micro=False))\n","    print(\"specificity \", self.specificity(), self.specificity(micro=False))\n","    print(\"f_measure \", self.f_measure(), self.f_measure(micro=False))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SUQfCKp2cnFq"},"source":["## save"]},{"cell_type":"code","metadata":{"id":"zjbwvKN0qRtM"},"source":["def path_with_time(path : str) -> str:\n","  dt_now = datetime.datetime.now(pytz.timezone('Asia/Tokyo'))\n","  return '{}-{}'.format(path, dt_now.strftime('%Y-%m-%d_%H-%M-%S'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lbL3qJOIomWe"},"source":["def create_exp_dir(path, scripts_to_save=None):\n","  if not os.path.exists(path):\n","    os.mkdir(path)\n","  print('Experiment dir : {}'.format(path))\n","\n","  if scripts_to_save is not None:\n","    os.mkdir(os.path.join(path, 'scripts'))\n","    for script in scripts_to_save:\n","      dst_file = os.path.join(path, 'scripts', os.path.basename(script))\n","      shutil.copyfile(script, dst_file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-1UaF37zpTFY"},"source":["def init_logging(save_path):\n","  class Formatter(logging.Formatter):\n","      \"\"\"override logging.Formatter to use an aware datetime object\"\"\"\n","      def converter(self, timestamp):\n","          dt = datetime.datetime.fromtimestamp(timestamp)\n","          tzinfo = pytz.timezone('Asia/Tokyo')\n","          return tzinfo.localize(dt)\n","          \n","      def formatTime(self, record, datefmt=None):\n","          dt = self.converter(record.created)\n","          if datefmt:\n","              s = dt.strftime(datefmt)\n","          else:\n","              try:\n","                  s = dt.isoformat(timespec='milliseconds')\n","              except TypeError:\n","                  s = dt.isoformat()\n","          return s\n","\n","  log_format = '%(asctime)s %(message)s'\n","  logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n","      format=log_format, datefmt='%m/%d %I:%M:%S %p')\n","  fh = logging.FileHandler(os.path.join(save_path, 'log.txt'))\n","  fh.setFormatter(Formatter(log_format))\n","  logging.getLogger().addHandler(fh)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bVnLFIMristi"},"source":["def save_dir(dir : str, drivepath = './drive/My Drive/ml'):\n","  # TODO : init for mount drive\n","  \n","  if not dir: return\n","\n","  import subprocess\n","  res = subprocess.run([\"cp\", \"-r\", \"./\" + dir, drivepath], stdout=subprocess.PIPE)\n","  sys.stdout.write(res.stdout)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fdtpwLbLudLR"},"source":["class Store():\n","  def __init__(self, dir=\"result\", name=\"log\", fig=[]):\n","    self.dict = {}\n","    self.dir = dir\n","    self.name = name\n","    self.fig = fig\n","  \n","  def add(self, name, value):\n","    if not name in self.dict:\n","      self.dict[name] = []\n","    self.dict[name].append(value)\n","  \n","  def update(self, store : 'Store'):\n","    self.dict.update(store.dict)\n","\n","  def save(self):\n","    self.save_log()\n","    for metrix, x, y in self.fig:\n","      self.save_fig(metrix, x, y)\n","\n","  def save_log(self, name=None):\n","    name = name if name else self.name\n","    path = os.path.join(self.dir, name + \".txt\")\n","    with open(path, mode='w') as f:\n","      f.write(\"%s\" % self.dict)\n","\n","  def save_fig(self, metrix, xlabel, ylabel, show=True):\n","    fig = plt.figure()\n","\n","    if type(metrix) is str:\n","      times = len(self.dict[metrix])\n","      plt.plot(np.arange(times), self.dict[metrix])\n","    else :\n","      times = len(self.dict[metrix[0]])\n","      for m in metrix:\n","        plt.plot(np.arange(times), self.dict[m], label=m)\n","      metrix = \"_\".join(metrix)\n","    \n","    plt.xlabel(xlabel)\n","    plt.ylabel(ylabel)\n","    plt.legend()\n","    if show: plt.show()\n","    fig.savefig(os.path.join(self.dir, \"%s_%d.png\" % (metrix, times)))\n","\n","  def __repr__(self):\n","    return \"store in %s\" % self.dict"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OUQoaMlybaLE"},"source":["# dataset"]},{"cell_type":"code","metadata":{"id":"aMQcdozPm31Z"},"source":["def load_dataset(train=2000, test=500, valid=0):\n","  #画像の変形処理\n","  transform = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n","  ])\n","\n","  transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","  ])\n","\n","  dataset = torchvision.datasets.CIFAR10\n","  kwargs = {\"root\" : \"./data\", \"download\" : True}\n","\n","  #CIFAR-10のtrain, testsetのロード\n","  trainset = dataset(train=True, transform=transform, **kwargs)\n","  testset = dataset(train=False, transform=transform_test, **kwargs)\n","  \n","  trainset, validset, _ = torch.utils.data.random_split(trainset, [train, valid, 50000-train-valid])\n","  testset, _ = torch.utils.data.random_split(testset, [test, 10000-test])\n","  return argparse.Namespace(train=trainset, test=testset, valid=validset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vNDJPZLxAY0B"},"source":["def load_dataloader(args):\n","  kwargs = {'num_workers': 1, 'pin_memory': True} if args.use_cuda else {}\n","  args.valid_size = args.valid_size if args.valid_size else 0\n","  sets = load_dataset(train=args.train_size, test=args.test_size, valid=args.valid_size)\n","  trainloader = torch.utils.data.DataLoader(sets.train, batch_size=args.batch_size, shuffle=True, **kwargs)\n","  validloader = torch.utils.data.DataLoader(sets.valid, batch_size=args.batch_size, shuffle=True, **kwargs)\n","  testloader = torch.utils.data.DataLoader(sets.test, batch_size=args.batch_size, shuffle=False, **kwargs)\n","  return argparse.Namespace(train=trainloader, test=testloader, valid=validloader)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nmXw5AGs0TLl"},"source":["# model"]},{"cell_type":"markdown","metadata":{"id":"TtrTvApwmZ-g"},"source":["## sampler"]},{"cell_type":"code","metadata":{"id":"kpAWtHGuMRog"},"source":["class ArchitectureSampler():\n","  def __call__(self, graph : nx.DiGraph, alpha : torch.Tensor) -> nx.DiGraph:\n","    return self.graph(graph, alpha)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2-BCzSJsOYI9"},"source":["class MaxSampler(ArchitectureSampler):\n","  def graph(self, graph, alpha):\n","    G = nx.DiGraph(graph)\n","    n = G.number_of_nodes()\n","\n","    for j in range(1, n):\n","      edges = [(i, j) for i in G.predecessors(j)]\n","      alphas = [alpha[i, j].item() for i, j in edges]\n","      edge_num = round(sum(alphas))\n","      disable = sorted(zip(edges, alphas), key=lambda x: x[-1])[:-edge_num]\n","      G.remove_edges_from([i for i, _ in disable])\n","\n","    return G"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FOHZG9qu0OPV"},"source":["class ForwardSampler(ArchitectureSampler):\n","  def graph(self, graph, alpha):\n","    G = nx.DiGraph(graph)\n","    n = G.number_of_nodes()\n","\n","    for j in range(1, n):\n","      edges = [(i, j) for i in G.predecessors(j) if not i + 1 == j]\n","      G.remove_edges_from(edges)\n","\n","    return G"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sgcYS7DpgVOQ"},"source":["class StrideCutSampler(ArchitectureSampler):\n","  def __init__(self, stride_max=8):\n","    assert stride_max >= 1\n","    self.stride = stride_max\n","\n","  def graph(self, graph, alpha):\n","    G = nx.DiGraph(graph)\n","    n = G.number_of_nodes()\n","\n","    for j in range(1, n):\n","      edges = [(i, j) for i in G.predecessors(j) if G.edges[i, j]['stride'] > self.stride]\n","      G.remove_edges_from(edges)\n","\n","    return G"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iG9p-K46g24-"},"source":["# model = load_model(dir=\"\", gene='VGG19')\n","# g = model.graph\n","# sampler = StrideCutSampler(2)\n","# h = model.sampled_graph(sampler)\n","# render_graph(h, 'graph')\n","# # nx.graph_edit_distance(g, h)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rmIlPTK1mUC3"},"source":["## module"]},{"cell_type":"code","metadata":{"id":"Hdd_Mu8gisSi"},"source":["cfg = {\n","    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n","    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n","}\n","\n","class VGG(nn.Module):\n","    def __init__(self, vgg_name):\n","        super(VGG, self).__init__()\n","        self.features = self._make_layers(cfg[vgg_name])\n","        self.classifier = nn.Linear(512, 10)\n","\n","    def forward(self, x):\n","        out = self.features(x)\n","        out = out.view(out.size(0), -1)\n","        out = self.classifier(out)\n","        return out\n","\n","    def _make_layers(self, cfg):\n","        layers = []\n","        in_channels = 3\n","        for x in cfg:\n","            if x == 'M':\n","                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","            else:\n","                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n","                           nn.BatchNorm2d(x),\n","                           nn.ReLU(inplace=True)]\n","                in_channels = x\n","        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n","        return nn.Sequential(*layers)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MrWmBPh2ylSx"},"source":["class FactorizedReduce(nn.Module):\n","  def __init__(self, channel_in, channel_out, stride, affine=True):\n","    super(FactorizedReduce, self).__init__()\n","    assert channel_out % stride == 0\n","    # self.relu = nn.ReLU(inplace=False)\n","    self.convs = nn.ModuleList([\n","      nn.Conv2d(channel_in, channel_out // stride, 1, stride=stride, padding=0, bias=False)\n","      for _ in range(stride)\n","    ])\n","    # self.conv_1 = nn.Conv2d(channel_in, channel_out // stride, 1, stride=stride, padding=0, bias=False)\n","    # self.conv_2 = nn.Conv2d(channel_in, channel_out // stride, 1, stride=stride, padding=0, bias=False) \n","    self.bn = nn.BatchNorm2d(channel_out, affine=affine)\n","\n","  def forward(self, x):\n","    # x = self.relu(x)\n","    # strideの偶奇による情報ロスを防ぐ\n","    # out = torch.cat([self.conv_1(x), self.conv_2(x[:,:,1:,1:])], dim=1)\n","    out = torch.cat([conv(x[:,:,i:,i:]) for i, conv in enumerate(self.convs)], dim=1)\n","    out = self.bn(out)\n","    return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y_uyawqz6UEv"},"source":["class Shortcut(nn.Module):\n","  def __init__(self, in_channel, out_channel, stride):\n","    super(Shortcut, self).__init__()\n","    self.f = self._shortcut(in_channel, out_channel, stride)\n","\n","  def forward(self, x):\n","    return self.f(x)\n","\n","  def _shortcut(self, channel_in, channel_out, stride):\n","    if stride > 1:\n","      return FactorizedReduce(channel_in, channel_out, stride)\n","    elif channel_in != channel_out:\n","      return nn.Conv2d(channel_in, channel_out, \n","                       kernel_size=1, stride=stride, padding=0)\n","    else:\n","      return lambda x: x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CHHLMEl5lNZt"},"source":["class Block(nn.Module):\n","  def __init__(self, graph, index):\n","    super(Block, self).__init__()\n","    node = graph.nodes[index]\n","    edges = [(i, index, graph.edges[i, index]) for i in graph.predecessors(index)]\n","\n","    self.index = index\n","    self.indices = [i for i, _, _ in edges]\n","    self.edges = nn.ModuleList([self._build_module(s) for i, j, s in edges])\n","\n","    process = [nn.ReLU(inplace=True)]\n","    if node['pool']: process += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","    self.post_process = nn.Sequential(*process)\n","\n","  def _build_module(self, setting):\n","    module = setting['module']\n","    in_channel, out_channel = setting['channel']\n","    stride = setting['stride']\n","    if module == 'forward':\n","      return nn.Sequential(\n","          nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1),\n","          nn.BatchNorm2d(out_channel)\n","          )\n","    elif module == 'shortcut':\n","      return Shortcut(in_channel, out_channel, stride)\n","    else:\n","      raise ValueError(\"module name\")\n","\n","  def forward(self, inputs, alpha):\n","    output = sum(alpha[i] * F(inputs[i]) for i, F in zip(self.indices, self.edges))\n","    return self.post_process(output)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hQ9-qdW5ko9R"},"source":["# 前提条件 : make graph, modules(Block, pool, ...)\n","# 拘束条件 : alpha sofmax\n","class Network(nn.Module):\n","  def __init__(self, gene, graph=None, preprocess : ArchitectureSampler=None):\n","    super(Network, self).__init__()\n","    self.gene = gene\n","    self.evaluate = bool(graph)\n","    self.graph = graph if graph else self._make_graph(gene)\n","    self.graph = preprocess(self.graph, None) if preprocess else self.graph\n","\n","    self.blocks = nn.ModuleList(self._make_blocks(self.graph))\n","    self.pool = nn.AvgPool2d(kernel_size=1, stride=1)\n","    self.classifier = nn.Linear(512, 10)\n","\n","  def _make_graph(self, gene, color_channel=3):\n","    \n","    def _decode_gene(gene):\n","      ch, st = [], []\n","      for g in gene:\n","        if g == 'M':\n","          st[-1] *= 2\n","        else :\n","          ch += [g]\n","          st += [1]\n","      return ch, st\n","\n","    def __pi(array):\n","      r = []\n","      s = 1\n","      for q in array:\n","        s *= q\n","        r += [s]\n","      return r\n","\n","    channel, stride = _decode_gene(gene)\n","    channel = [color_channel] + channel\n","    stride = [1] + stride\n","    s_stride = __pi(stride)\n","\n","    n = len(channel)\n","    nodes = [(i, {'channel':channel[i], 'stride':s_stride[i], 'pool':stride[i]>1}) for i in range(n)]\n","    nodes[0][-1].update({'name':'input'})\n","    edges = [(i, j, {}) for i in range(n) for j in range(n) if i < j and not (i == 0 and j > 1)]\n","    for (i, j, d) in edges:\n","      d.update({\n","        'module' : 'forward' if i + 1 == j else 'shortcut', \n","        'channel' : (nodes[i][-1]['channel'], nodes[j][-1]['channel']),\n","        'stride' : int(nodes[j-1][-1]['stride'] / nodes[i][-1]['stride'])\n","      })\n","\n","    G = nx.DiGraph()\n","    G.add_nodes_from(nodes)\n","    G.add_edges_from(edges)\n","    return G\n","\n","  def _make_blocks(self, graph):\n","    return [Block(graph, i) for i in graph.nodes() if i > 0]\n","\n","  def init_alpha(self, device):\n","    def _init_alpha(node_num, device, delta=1e-3):\n","      noise = delta * torch.randn(node_num, node_num, device=device)\n","      alpha = noise.clone().detach().requires_grad_(True)\n","      return [alpha]\n","\n","    def _mask(node_num, device, name):\n","      mask = torch.zeros(node_num, node_num, device=device)\n","\n","      for i, j in self.graph.edges():\n","        op = self.graph.edges[i, j]['module']\n","        if not op == name: continue\n","\n","        mask[i, j] = 1\n","\n","      return mask.t() > 0\n","\n","    n = self.graph.number_of_nodes()\n","    self.alphas = _init_alpha(n, device)\n","    self.mask_s = _mask(n, device, 'shortcut')\n","    self.mask_f = _mask(n, device, 'forward')\n","\n","    return self\n","\n","  def normalized_alpha(self):\n","    alpha = torch.zeros_like(self.alphas[0])\n","    if self.evaluate:\n","      for i, j in self.graph.edges():\n","        alpha[j, i] = 1.0\n","    else:\n","      alpha[self.mask_f] = 1.0\n","      for a, raw, mask in zip(alpha, self.alphas[0], self.mask_s):\n","        a[mask] = F.softmax(raw[mask], dim=0)\n","    return alpha\n","\n","  def sampled_graph(self, sampler : ArchitectureSampler):\n","    return sampler(self.graph, self.matrix_alpha())\n","\n","  @torch.no_grad()\n","  def matrix_alpha(self, normalize=True):\n","    return (self.normalized_alpha() if normalize else self.alphas[0]).t()\n","\n","\n","  def forward(self, x):\n","    state = [x]\n","    alpha = self.normalized_alpha()\n","\n","    for block in self.blocks:\n","      x = block(state, alpha[block.index])\n","      state += [x]\n","\n","    out = self.pool(x)\n","    out = out.view(out.size(0), -1)\n","    out = self.classifier(out)\n","    return out"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gii9_gAMkEyV"},"source":["## others"]},{"cell_type":"code","metadata":{"id":"F4Q7p8JyXk9f"},"source":["class Architect():\n","  def __init__(self, valid_loader, model, criterion, lr, device):\n","    self.valid_loader = valid_loader\n","    self.model = model\n","    self.criterion = criterion\n","    self.optimizer = optim.Adam(model.alphas, lr=lr, betas=(0.5, 0.999), weight_decay=1e-3)\n","    self.device = device\n","\n","  def step(self):\n","    data_v, target_v = next(iter(self.valid_loader))\n","    data_v, target_v = data_v.to(self.device), target_v.to(self.device)\n","\n","    self.optimizer.zero_grad()\n","    output = self.model(data_v)\n","    loss = self.criterion(output, target_v)\n","    loss.backward()\n","    self.optimizer.step()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oGBb6LPrS4cM"},"source":["def render_graph(graph, path):\n","  format = dict(\n","      format='png', \n","      edge_attr=dict(fontsize='20', fontname=\"times\"),\n","      node_attr=dict(style='filled', shape='rect', align='center', fontsize='20', height='0.5', width='0.5', penwidth='2', fontname=\"times\"),\n","      engine='dot' # circo, dot, fdp, neato, osage, sfdp, twopi\n","  )\n","\n","  dg = graphviz.Digraph(**format)\n","\n","  dg.attr('node', fillcolor='dodgerblue4', fontcolor='white', fontsize='15') # coral, \n","  for node in graph.nodes():\n","    attr = graph.nodes[node]\n","    label = attr['name'] if 'name' in attr else str(node)\n","    label += '\\n(%s, %d, %d)' % (attr['channel'], 32 / attr['stride'], 32 / attr['stride'])\n","    dg.node(str(node), label=label)\n","\n","  for (i, j) in graph.edges():\n","    attr = graph.edges[i, j]\n","    label = attr['module']\n","    label = \"\"\n","    style = 'bold' if attr['module'] == 'forward' else 'dashed'\n","    dg.edge(str(i), str(j), label=label, style=style)\n","\n","  dg.render(path)\n","  return dg"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"26E8hA820WWG"},"source":["# learning"]},{"cell_type":"markdown","metadata":{"id":"pmMw2Fhi2gdl"},"source":["## events"]},{"cell_type":"code","metadata":{"id":"xKYJMQebTseQ"},"source":["@Experiment.event('setup')\n","def setup(args):\n","  args.save = path_with_time(args.save)\n","  create_exp_dir(args.save, scripts_to_save=glob.glob('*.py'))\n","  init_logging(args.save)\n","  logging.info('kwargs %s' % args)\n","  \n","  args.start_epoch = 0\n","\n","  # log init\n","  fig = [([\"train_acc\", \"test_acc\"], \"epochs\", \"accuracy[%]\"),\n","         ([\"train_loss\", \"test_loss\"], \"epochs\", \"loss\")]\n","  store = Store(dir=args.save, name=\"store\", fig=fig)\n","  args.store = store\n","\n","  set_seed(args.seed)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M-3CxbXTE7MN"},"source":["@Experiment.event('setup')\n","def set_device(args):\n","  # cuda init\n","  args.use_cuda = torch.cuda.is_available()\n","  args.device = torch.device(\"cuda\" if args.use_cuda else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sjuq4RUvWGnU"},"source":["@Experiment.event('checkpoint', 'end', order=1)\n","def save_checkpoint(args):\n","  args.store.save()\n","  save_dir(args.save)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CNo6kmtFi1bh"},"source":["@argspace(retain_graph=True)\n","def train(args):\n","  objs = AvgrageMeter()\n","  top1 = AvgrageMeter()\n","  top5 = AvgrageMeter()\n","  args.model.train()\n","\n","  for step, (input, target) in enumerate(args.dataset):\n","    n = input.size(0)\n","\n","    input = Variable(input, requires_grad=False).to(args.device)\n","    target = Variable(target, requires_grad=False).to(args.device)\n","\n","    args.architect.step()\n","\n","    args.optimizer.zero_grad()\n","    logits = args.model(input)\n","    loss = args.criterion(logits, target)\n","\n","    loss.backward(retain_graph=args.retain_graph)\n","    # nn.utils.clip_grad_norm(model.parameters(), args.grad_clip)\n","    args.optimizer.step()\n","\n","    prec1, prec5 = accuracy(logits, target, topk=(1, 5))\n","    objs.update(loss.item(), n)\n","    top1.update(prec1.item(), n)\n","    top5.update(prec5.item(), n)\n","\n","    if step % args.report_freq == 0:\n","      logging.info('train %03d %e %f %f', step, objs.avg, top1.avg, top5.avg)\n","\n","  return top1.avg, objs.avg"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xQKeYZkwi2Dx"},"source":["@argspace()\n","def infer(args):\n","  objs = AvgrageMeter()\n","  top1 = AvgrageMeter()\n","  top5 = AvgrageMeter()\n","  args.model.eval()\n","\n","  for step, (input, target) in enumerate(args.dataset):\n","    input = Variable(input, requires_grad=False).to(args.device)\n","    target = Variable(target, requires_grad=False).to(args.device)\n","\n","    logits = args.model(input)\n","    loss = args.criterion(logits, target)\n","\n","    prec1, prec5 = accuracy(logits, target, topk=(1, 5))\n","    n = input.size(0)\n","    objs.update(loss.item(), n)\n","    top1.update(prec1.item(), n)\n","    top5.update(prec5.item(), n)\n","\n","    if step % args.report_freq == 0:\n","      logging.info('valid %03d %e %f %f', step, objs.avg, top1.avg, top5.avg)\n","\n","  return top1.avg, objs.avg"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"59baVbbR2pCv"},"source":["## experiment"]},{"cell_type":"code","metadata":{"id":"yHLra87VEQ1P"},"source":["@argspace(gene='VGG11', stride_max=0, graph=None, dir=None)\n","def load_model(args):\n","  \n","  # init\n","  set_device(args)\n","  device = args.device\n","\n","  # model setup\n","  gene = cfg[args.gene]\n","  sampler = StrideCutSampler(args.stride_max) if args.stride_max > 0 else None\n","  model = Network(gene, graph=args.graph, preprocess=sampler).to(device).init_alpha(device)\n","\n","  # resume\n","  if args.dir:\n","    state = torch.load(os.path.join(args.dir, 'checkpoint.pth'))\n","    model.load_state_dict(state['model'])\n","    model.alphas = state['alpha']\n","\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tTUPG9pfq20n"},"source":["@argspace(gene='VGG11', stride_max=0)\n","def main(args):\n","\n","  # init\n","  exp = Experiment()\n","  exp('setup')(args)\n","  store = args.store\n","  device = args.device\n","\n","  # model setup\n","  gene = cfg[args.gene]\n","  logging.info('gene %s', gene)\n","  sampler = StrideCutSampler(args.stride_max) if args.stride_max > 0 else None\n","  graph = args.graph if args.graph else None\n","  model = Network(gene, graph=graph, preprocess=sampler).to(device).init_alpha(device)\n","\n","  # load cuda\n","  if device == 'cuda':\n","    model = torch.nn.DataParallel(model)\n","    cudnn.benchmark = True\n","\n","  # resume\n","  if args.dir:\n","    state = torch.load(os.path.join(args.dir, 'checkpoint.pth'))\n","    model.load_state_dict(state['model'])\n","    model.alphas = state['alpha']\n","    args.start_epoch = state['epoch']\n","    store.update(state['store'])\n","    logging.info('Resuming from epoch %d in %s' % (args.start_epoch, args.dir))\n","\n","  loader = load_dataloader(args)\n","\n","  optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=5e-4)\n","  criterion = nn.CrossEntropyLoss()\n","  architect = Architect(loader.valid, model, criterion, args.lr_alpha, device)\n","\n","  @Experiment.event('checkpoint', 'end')\n","  def save_model(args):\n","    state = {\n","      'model': model.state_dict(),\n","      'graph': model.graph,\n","      'alpha': model.alphas,\n","      'store': store,\n","      'epoch': epoch,\n","    }\n","    torch.save(state, os.path.join(args.save, 'checkpoint.pth'))\n","    \n","  @Experiment.event('epoch_end')\n","  def save_graph(args):\n","    sampler = MaxSampler()\n","    graph = model.sampled_graph(sampler)\n","    torch.save(graph, os.path.join(args.save, 'graph_%d' % epoch + '.pth'))\n","    render_graph(graph, os.path.join(args.save, 'graph_%d' % epoch))\n","\n","\n","  for epoch in range(args.start_epoch + 1, args.epochs + 1):\n","    logging.info('epoch %d', epoch)\n","    logging.info('alpha %s', model.matrix_alpha())\n","\n","    # training\n","    train_acc, train_obj = train(dataset=loader.train, model=model, \n","                                 criterion=criterion, optimizer=optimizer, \n","                                 architect=architect, \n","                                 device=device, report_freq=args.report)\n","    logging.info('train_acc %f', train_acc)\n","    store.add(\"train_loss\", train_obj)\n","    store.add(\"train_acc\", train_acc)\n","\n","    # validation\n","    valid_acc, valid_obj = infer(dataset=loader.test, model=model, \n","                                 criterion=criterion,\n","                                 device=device, report_freq=args.report)\n","    logging.info('valid_acc %f', valid_acc)\n","    store.add(\"test_loss\", valid_obj)\n","    store.add(\"test_acc\", valid_acc)\n","\n","    exp('epoch_end')(args)\n","\n","    if epoch % args.checkpoint == 0:\n","      exp('checkpoint')(args)\n","\n","  exp('end')(args)\n","\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nGjJsHh50xtj"},"source":["# main"]},{"cell_type":"code","metadata":{"id":"uU-CpicQwsAF"},"source":["if __name__ == '__main__':\n","  main(save=\"exp_vgg19\", lr=0.01, momentum=0.9, lr_alpha=0.003, epochs=50, \n","       train_size=25000, valid_size=25000, test_size=5000, batch_size=64,\n","       gene='VGG19', stride_max=2,\n","       seed=41, report=100, checkpoint=10, dir=\"\", graph=None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OkKv4_eo-Fhh"},"source":["if __name__ == '__main__':\n","  args = {'gene':'VGG19', 'stride_max':2}\n","  model = load_model(dir=\"exp_vgg19-2020-09-24_15-22-36\", **args)\n","  \n","  sampler : ArchitectureSampler = MaxSampler()\n","  graph = model.sampled_graph(sampler)\n","  main(save=\"exp_vgg19_eval\", lr=0.01, momentum=0.9, lr_alpha=0.003, epochs=100, \n","       train_size=25000, valid_size=25000, test_size=5000, batch_size=64,\n","       seed=41, report=100, checkpoint=10, dir=\"\", graph=graph, **args)\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f4BFc-WUjat9"},"source":["if __name__ == '__main__':\n","  model = load_model(dir=\"exp-2020-09-24_08-53-44\")\n","  \n","  # sampler : ArchitectureSampler = ForwardSampler()\n","  # graph = model.sampled_graph(sampler)\n","  # main(save=\"exp_eval_forward\", lr=0.01, momentum=0.9, lr_alpha=0.0, epochs=100, \n","  #     train_size=25000, valid_size=25000, test_size=5000, batch_size=64, \n","  #     seed=41, report=100, checkpoint=10, dir=\"\", graph=graph)\n","  \n","  sampler : ArchitectureSampler = MaxSampler()\n","  graph = model.sampled_graph(sampler)\n","  main(save=\"exp_eval_max\", lr=0.01, momentum=0.9, lr_alpha=0.0, epochs=100, \n","      train_size=25000, valid_size=25000, test_size=5000, batch_size=64, \n","      seed=41, report=100, checkpoint=10, dir=\"\", graph=graph)\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EuByAf2cLRoS"},"source":["# exp"]},{"cell_type":"markdown","metadata":{"id":"s--FDQ74LxBJ"},"source":["TODO\n","- watch model size, memory\n","- early stopping"]},{"cell_type":"code","metadata":{"id":"rHZ2XSodc0JH"},"source":[""],"execution_count":null,"outputs":[]}]}