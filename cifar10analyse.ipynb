{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cifar10analyse.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"eEoEfZteii7G","colab_type":"code","colab":{}},"source":["import os\n","import sys\n","import time\n","import glob\n","import json\n","import numpy as np\n","import logging\n","import argparse\n","\n","import datetime\n","import pytz\n","\n","import torch #基本モジュール\n","from torch.autograd import Variable #自動微分用\n","import torch.nn as nn #ネットワーク構築用\n","import torch.optim as optim #最適化関数\n","import torch.nn.functional as F #ネットワーク用の様々な関数\n","import torch.utils.data #データセット読み込み関連\n","import torchvision #画像関連\n","from torchvision import datasets, models, transforms #画像用データセット諸々\n","import torch.backends.cudnn as cudnn\n","\n","from google.colab import files"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LWsydO_m0MtN","colab_type":"text"},"source":["# utils"]},{"cell_type":"code","metadata":{"id":"p87M0wP1lJRO","colab_type":"code","colab":{}},"source":["def dictspace(f):\n","  def inner(**kwds):\n","    return f(argparse.Namespace(**kwds))\n","  return inner"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZCJioEPw2esA","colab_type":"code","colab":{}},"source":["def dictspace_default(**kwds):\n","  def deco(f):\n","    def inner(**ikwds):\n","      kwds.update(ikwds)\n","      return f(argparse.Namespace(**kwds))\n","    return inner\n","  return deco"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CJ28A29Kjm_4","colab_type":"code","colab":{}},"source":["def accuracy(output, target, topk=(1,)):\n","  maxk = max(topk)\n","  batch_size = target.size(0)\n","\n","  _, pred = output.topk(maxk, 1, True, True)\n","  pred = pred.t()\n","  correct = pred.eq(target.view(1, -1).expand_as(pred))\n","\n","  res = []\n","  for k in topk:\n","    correct_k = correct[:k].view(-1).float().sum(0)\n","    res.append(correct_k.mul_(100.0/batch_size))\n","  return res"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZZUgjfdokzq8","colab_type":"code","colab":{}},"source":["class AvgrageMeter(object):\n","\n","  def __init__(self):\n","    self.reset()\n","\n","  def reset(self):\n","    self.avg = 0\n","    self.sum = 0\n","    self.cnt = 0\n","\n","  def update(self, val, n=1):\n","    self.sum += val * n\n","    self.cnt += n\n","    self.avg = self.sum / self.cnt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zjbwvKN0qRtM","colab_type":"code","colab":{}},"source":["def save_path(path):\n","  dt_now = datetime.datetime.now(pytz.timezone('Asia/Tokyo'))\n","  return '{}-{}'.format(path, dt_now.strftime('%Y-%m-%d_%H-%M-%S'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lbL3qJOIomWe","colab_type":"code","colab":{}},"source":["def create_exp_dir(path, scripts_to_save=None):\n","  if not os.path.exists(path):\n","    os.mkdir(path)\n","  print('Experiment dir : {}'.format(path))\n","\n","  if scripts_to_save is not None:\n","    os.mkdir(os.path.join(path, 'scripts'))\n","    for script in scripts_to_save:\n","      dst_file = os.path.join(path, 'scripts', os.path.basename(script))\n","      shutil.copyfile(script, dst_file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-1UaF37zpTFY","colab_type":"code","colab":{}},"source":["def init_logging(save_path):\n","  class Formatter(logging.Formatter):\n","      \"\"\"override logging.Formatter to use an aware datetime object\"\"\"\n","      def converter(self, timestamp):\n","          dt = datetime.datetime.fromtimestamp(timestamp)\n","          tzinfo = pytz.timezone('Asia/Tokyo')\n","          return tzinfo.localize(dt)\n","          \n","      def formatTime(self, record, datefmt=None):\n","          dt = self.converter(record.created)\n","          if datefmt:\n","              s = dt.strftime(datefmt)\n","          else:\n","              try:\n","                  s = dt.isoformat(timespec='milliseconds')\n","              except TypeError:\n","                  s = dt.isoformat()\n","          return s\n","\n","  log_format = '%(asctime)s %(message)s'\n","  logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n","      format=log_format, datefmt='%m/%d %I:%M:%S %p')\n","  fh = logging.FileHandler(os.path.join(save_path, 'log.txt'))\n","  fh.setFormatter(Formatter(log_format))\n","  logging.getLogger().addHandler(fh)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6ml84YYEzZ_4","colab_type":"code","colab":{}},"source":["def set_seed(seed):\n","  np.random.seed(seed)\n","  # cudnn.benchmark = True\n","  torch.manual_seed(seed)\n","  # cudnn.enabled=True\n","  torch.cuda.manual_seed(seed)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pl4-bpDUIC8Z","colab_type":"code","colab":{}},"source":["class ConfusionMatrix():\n","  def __init__(self):\n","    pass\n","  \n","  # @torch.no_grad()\n","  # def run(self, model, dataloader):\n","  #   data, target = next(iter(dataloader))\n","  #   device = data.device\n","  #   num = target.max().long() + 1\n","  #   cm = torch.zeros(num, num).to(device)\n","\n","  #   for i, (data, target) in enumerate(dataloader):\n","  #     data = data.to(device)\n","  #     target = target.to(device)\n","  #     outputs = model(data)\n","  #     _, preds = torch.max(outputs, 1)\n","  #     for t, p in zip(target.view(-1), preds.view(-1)):\n","  #       cm[t.long(), p.long()] += 1\n","\n","  #   self.matrix = cm\n","  #   self.dim = num\n","  #   self.sum0 = self.matrix.sum(0)\n","  #   self.sum1 = self.matrix.sum(1)\n","  #   self.sum = self.matrix.sum()\n","  #   return cm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5HIFrNph_P8t","colab_type":"code","colab":{}},"source":["class Metrics():\n","  def __init__(self, model, dataloader):\n","    self._run(model, dataloader)\n","    self.epsilon = 1e-7\n","  \n","  @torch.no_grad()\n","  def _run(self, model, dataloader):\n","    data, target = next(iter(dataloader))\n","    device =  next(model.parameters()).device\n","    print(device)\n","    num = target.max().long() + 1\n","    cm = torch.zeros(num, num).to(device)\n","\n","    for i, (data, target) in enumerate(dataloader):\n","      data = data.to(device)\n","      target = target.to(device)\n","      outputs = model(data)\n","      _, preds = torch.max(outputs, 1)\n","      for t, p in zip(target.view(-1), preds.view(-1)):\n","        cm[t.long(), p.long()] += 1\n","\n","    self.matrix = cm\n","    self.dim = num\n","    self.sum0 = self.matrix.sum(0)\n","    self.sum1 = self.matrix.sum(1)\n","    self.sum = self.matrix.sum()\n","\n","  def confusion_matrix(self):\n","    return self.matrix\n","\n","  def TP(self, index):\n","    return self.matrix[index][index]\n","\n","  def FN(self, index):\n","    return self.sum1[index] - self.TP(index)\n","\n","  def FP(self, index):\n","    return self.sum0[index] - self.TP(index)\n","\n","  def TN(self, index):\n","    return self.sum - self.TP(index) - self.FN(index) - self.FP(index)\n","\n","  def _sum(self, F):\n","    return sum(F(i) for i in range(self.dim))\n","    \n","  def _micro(self, F, G):\n","    return self._sum(F) / (self._sum(F) + self._sum(G) + self.epsilon)\n","\n","  def _macro(self, F, G):\n","    return sum(F(i) / (F(i) + G(i) + self.epsilon) for i in range(self.dim)) / self.dim\n","\n","  def _switch(self, F, G, micro):\n","    return (self._micro(F, G) if micro else self._macro(F, G))\n","\n","  def accuracy(self, micro=True):\n","    return (self._sum(self.TP) / self.sum if micro else \n","            (sum(self.TP(i) / self.sum1 for i in range(self.dim)) / self.dim).mean())\n","\n","  def precision(self, micro=True):\n","    return self._switch(self.TP, self.FP, micro)\n","\n","  def recall(self, micro=True):\n","    return self._switch(self.TP, self.FN, micro)\n","\n","  def specificity(self, micro=True):\n","    return self._switch(self.TN, self.FP, micro)\n","\n","  def f_measure(self, micro=True):\n","    p, r = self.precision(micro), self.recall(micro)\n","    return 2 * p * r / (p + r + self.epsilon)\n","\n","  def print(self):\n","    print(self.confusion_matrix())\n","    print(\"accuracy \", self.accuracy(), self.accuracy(micro=False))\n","    print(\"precision \", self.precision(), self.precision(micro=False))\n","    print(\"recall \", self.recall(), self.recall(micro=False))\n","    print(\"specificity \", self.specificity(), self.specificity(micro=False))\n","    print(\"f_measure \", self.f_measure(), self.f_measure(micro=False))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nmXw5AGs0TLl","colab_type":"text"},"source":["# model"]},{"cell_type":"code","metadata":{"id":"Hdd_Mu8gisSi","colab_type":"code","colab":{}},"source":["cfg = {\n","    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n","    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n","}\n","\n","class VGG(nn.Module):\n","    def __init__(self, vgg_name):\n","        super(VGG, self).__init__()\n","        self.features = self._make_layers(cfg[vgg_name])\n","        self.classifier = nn.Linear(512, 100)\n","\n","    def forward(self, x):\n","        out = self.features(x)\n","        out = out.view(out.size(0), -1)\n","        out = self.classifier(out)\n","        return out\n","\n","    def _make_layers(self, cfg):\n","        layers = []\n","        in_channels = 3\n","        for x in cfg:\n","            if x == 'M':\n","                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","            else:\n","                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n","                           nn.BatchNorm2d(x),\n","                           nn.ReLU(inplace=True)]\n","                in_channels = x\n","        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n","        return nn.Sequential(*layers)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ihr65VPv4pQZ","colab_type":"code","colab":{}},"source":["class Bottleneck(nn.Module):\n","    def __init__(self, last_planes, in_planes, out_planes, dense_depth, stride, first_layer):\n","        super(Bottleneck, self).__init__()\n","        self.out_planes = out_planes\n","        self.dense_depth = dense_depth\n","\n","        self.conv1 = nn.Conv2d(last_planes, in_planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv2 = nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=stride, padding=1, groups=32, bias=False)\n","        self.bn2 = nn.BatchNorm2d(in_planes)\n","        self.conv3 = nn.Conv2d(in_planes, out_planes+dense_depth, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(out_planes+dense_depth)\n","\n","        self.shortcut = nn.Sequential()\n","        if first_layer:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(last_planes, out_planes+dense_depth, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(out_planes+dense_depth)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        x = self.shortcut(x)\n","        d = self.out_planes\n","        out = torch.cat([x[:,:d,:,:]+out[:,:d,:,:], x[:,d:,:,:], out[:,d:,:,:]], 1)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class DPN(nn.Module):\n","    def __init__(self, cfg):\n","        super(DPN, self).__init__()\n","        in_planes, out_planes = cfg['in_planes'], cfg['out_planes']\n","        num_blocks, dense_depth = cfg['num_blocks'], cfg['dense_depth']\n","\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.last_planes = 64\n","        self.layer1 = self._make_layer(in_planes[0], out_planes[0], num_blocks[0], dense_depth[0], stride=1)\n","        self.layer2 = self._make_layer(in_planes[1], out_planes[1], num_blocks[1], dense_depth[1], stride=2)\n","        self.layer3 = self._make_layer(in_planes[2], out_planes[2], num_blocks[2], dense_depth[2], stride=2)\n","        self.layer4 = self._make_layer(in_planes[3], out_planes[3], num_blocks[3], dense_depth[3], stride=2)\n","        self.linear = nn.Linear(out_planes[3]+(num_blocks[3]+1)*dense_depth[3], 100)\n","\n","    def _make_layer(self, in_planes, out_planes, num_blocks, dense_depth, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for i,stride in enumerate(strides):\n","            layers.append(Bottleneck(self.last_planes, in_planes, out_planes, dense_depth, stride, i==0))\n","            self.last_planes = out_planes + (i+2) * dense_depth\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = F.avg_pool2d(out, 4)\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return out\n","\n","\n","def DPN26():\n","    cfg = {\n","        'in_planes': (96,192,384,768),\n","        'out_planes': (256,512,1024,2048),\n","        'num_blocks': (2,2,2,2),\n","        'dense_depth': (16,32,24,128)\n","    }\n","    return DPN(cfg)\n","\n","def DPN92():\n","    cfg = {\n","        'in_planes': (96,192,384,768),\n","        'out_planes': (256,512,1024,2048),\n","        'num_blocks': (3,4,20,3),\n","        'dense_depth': (16,32,24,128)\n","    }\n","    return DPN(cfg)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aMQcdozPm31Z","colab_type":"code","colab":{}},"source":["def load_dataset(train=2000, test=500, valid=0):\n","  #画像の変形処理\n","  transform = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n","  ])\n","\n","  transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","  ])\n","\n","  dataset = torchvision.datasets.CIFAR100\n","  kwargs = {\"root\" : \"./data\", \"download\" : True}\n","\n","  #CIFAR-10のtrain, testsetのロード\n","  trainset = dataset(train=True, transform=transform, **kwargs)\n","  testset = dataset(train=False, transform=transform_test, **kwargs)\n","  \n","  trainset, validset, _ = torch.utils.data.random_split(trainset, [train, valid, 50000-train-valid])\n","  testset, _ = torch.utils.data.random_split(testset, [test, 10000-test])\n","  return argparse.Namespace(train=trainset, test=testset, valid=validset)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"26E8hA820WWG","colab_type":"text"},"source":["# learning"]},{"cell_type":"code","metadata":{"id":"CNo6kmtFi1bh","colab_type":"code","colab":{}},"source":["@dictspace\n","def train(args):\n","  objs = AvgrageMeter()\n","  top1 = AvgrageMeter()\n","  top5 = AvgrageMeter()\n","  args.model.train()\n","\n","  for step, (input, target) in enumerate(args.dataset):\n","    n = input.size(0)\n","\n","    input = Variable(input, requires_grad=False).to(args.device)\n","    target = Variable(target, requires_grad=False).to(args.device)\n","\n","    args.optimizer.zero_grad()\n","    logits = args.model(input)\n","    loss = args.criterion(logits, target)\n","\n","    loss.backward()\n","    # nn.utils.clip_grad_norm(model.parameters(), args.grad_clip)\n","    args.optimizer.step()\n","\n","    prec1, prec5 = accuracy(logits, target, topk=(1, 5))\n","    objs.update(loss.item(), n)\n","    top1.update(prec1.item(), n)\n","    top5.update(prec5.item(), n)\n","\n","    if step % args.report_freq == 0:\n","      logging.info('train %03d %e %f %f', step, objs.avg, top1.avg, top5.avg)\n","\n","  return top1.avg, objs.avg"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xQKeYZkwi2Dx","colab_type":"code","colab":{}},"source":["@dictspace\n","def infer(args):\n","  objs = AvgrageMeter()\n","  top1 = AvgrageMeter()\n","  top5 = AvgrageMeter()\n","  args.model.eval()\n","\n","  for step, (input, target) in enumerate(args.dataset):\n","    input = Variable(input, requires_grad=False).to(args.device)\n","    target = Variable(target, requires_grad=False).to(args.device)\n","\n","    logits = args.model(input)\n","    loss = args.criterion(logits, target)\n","\n","    prec1, prec5 = accuracy(logits, target, topk=(1, 5))\n","    n = input.size(0)\n","    objs.update(loss.item(), n)\n","    top1.update(prec1.item(), n)\n","    top5.update(prec5.item(), n)\n","\n","    if step % args.report_freq == 0:\n","      logging.info('valid %03d %e %f %f', step, objs.avg, top1.avg, top5.avg)\n","\n","  return top1.avg, objs.avg"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PD2xPMC1ZC6P","colab_type":"text"},"source":["### train, infer\n","- model setup\n","- metrics setup\n","- dataset loop(data, cuda, loss, metrics, log)\n","- return metrics\n","\n","#### event\n","- awake\n","- main loop\n","- report loop\n","\n","\n","### learning\n","- device\n","- seed\n","- instantiate(model, opt, crite)\n","- if cuda\n","- dataloader\n","- epoch loop(log, train, test)\n","\n","### main\n","- save dir\n","- mkdir save\n","- init log\n","- call learning"]},{"cell_type":"code","metadata":{"id":"tTUPG9pfq20n","colab_type":"code","colab":{}},"source":["@dictspace\n","def learning(args):\n","\n","  use_cuda = torch.cuda.is_available()\n","  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","  set_seed(args.seed)\n","\n","  # instantiate\n","  model = DPN92().to(device)\n","  optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=5e-4)\n","  criterion = nn.CrossEntropyLoss()\n","\n","\n","  if device == 'cuda':\n","    model = torch.nn.DataParallel(model)\n","    cudnn.benchmark = True\n","\n","  kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n","  sets = load_dataset(train=args.train_size, test=args.test_size)\n","  trainloader = torch.utils.data.DataLoader(sets.train, batch_size=args.batch_size, shuffle=True, **kwargs)\n","  testloader = torch.utils.data.DataLoader(sets.test, batch_size=args.batch_size, shuffle=False, **kwargs)\n","  \n","  for epoch in range(args.epochs):\n","    logging.info('epoch %d', epoch)\n","\n","    # training\n","    train_acc, train_obj = train(dataset=trainloader, model=model, \n","                                 criterion=criterion, optimizer=optimizer, \n","                                 device=device, report_freq=args.report)\n","    logging.info('train_acc %f', train_acc)\n","\n","    # validation\n","    valid_acc, valid_obj = infer(dataset=testloader, model=model, \n","                                 criterion=criterion,\n","                                 device=device, report_freq=args.report)\n","    logging.info('valid_acc %f', valid_acc)\n","\n","    # m = Metrics(model, testloader)\n","    # m.print()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nGjJsHh50xtj","colab_type":"text"},"source":["# main"]},{"cell_type":"code","metadata":{"id":"L8ZJ1Mk8i3Lp","colab_type":"code","colab":{}},"source":["@dictspace\n","def main(args):\n","  args.save = save_path(args.save)\n","  create_exp_dir(args.save, scripts_to_save=glob.glob('*.py'))\n","  init_logging(args.save)\n","\n","  learning(**vars(args))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f4BFc-WUjat9","colab_type":"code","colab":{}},"source":["if __name__ == '__main__':\n","  main(exp=\"optuna\", save=\"exp\", lr=0.01, momentum=0.9, train_size=50000,\n","       test_size=5000, batch_size=64, epochs=100, seed=41, report=100)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YfnROzhgDEGp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1599641281914,"user_tz":-540,"elapsed":37654,"user":{"displayName":"Yy Yy","photoUrl":"","userId":"02437050620110452344"}},"outputId":"94d6d471-1b26-48bb-cc26-08d81898aee6"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","4/3wGy228D0GXKYw8w2PEIfHcIJ9M3CBEyBwzuHiH15faiRPO6WaGnL7U\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EuByAf2cLRoS","colab_type":"text"},"source":["# exp"]},{"cell_type":"markdown","metadata":{"id":"s--FDQ74LxBJ","colab_type":"text"},"source":["TODO\n","- google drive mount\n","- F1 Score\n","- Transfar learning\n","- decorate injection\n","- save model, checkpoint\n","- log store\n","- watch model size, memory\n","- early stopping"]},{"cell_type":"code","metadata":{"id":"7JnZ64gMg13Z","colab_type":"code","colab":{}},"source":["class F1_Loss(nn.Module):\n","    '''Calculate F1 score. Can work with gpu tensors\n","    \n","    The original implmentation is written by Michal Haltuf on Kaggle.\n","    \n","    Returns\n","    -------\n","    torch.Tensor\n","        `ndim` == 1. epsilon <= val <= 1\n","    \n","    Reference\n","    ---------\n","    - https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n","    - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n","    - https://discuss.pytorch.org/t/calculating-precision-recall-and-f1-score-in-case-of-multi-label-classification/28265/6\n","    - http://www.ryanzhang.info/python/writing-your-own-loss-function-module-for-pytorch/\n","    '''\n","    def __init__(self, epsilon=1e-7):\n","        super().__init__()\n","        self.epsilon = epsilon\n","        \n","    def forward(self, y_pred, y_true,):\n","        assert y_pred.ndim == 2\n","        assert y_true.ndim == 1\n","        y_true = F.one_hot(y_true, 2).to(torch.float32)\n","        y_pred = F.softmax(y_pred, dim=1)\n","        \n","        tp = (y_true * y_pred).sum(dim=0).to(torch.float32)\n","        tn = ((1 - y_true) * (1 - y_pred)).sum(dim=0).to(torch.float32)\n","        fp = ((1 - y_true) * y_pred).sum(dim=0).to(torch.float32)\n","        fn = (y_true * (1 - y_pred)).sum(dim=0).to(torch.float32)\n","\n","        precision = tp / (tp + fp + self.epsilon)\n","        recall = tp / (tp + fn + self.epsilon)\n","\n","        f1 = 2* (precision*recall) / (precision + recall + self.epsilon)\n","        f1 = f1.clamp(min=self.epsilon, max=1-self.epsilon)\n","        return 1 - f1.mean()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"opM705D_g4U0","colab_type":"code","colab":{}},"source":["\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","model = CNN9()\n","\n","sets = load_dataset()\n","kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n","testloader = torch.utils.data.DataLoader(sets.test, batch_size=64, **kwargs)\n","\n","F1_Loss()(model, testloader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6mYJ4ThpCAOz","colab_type":"code","colab":{}},"source":["\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","model = CNN9()\n","\n","sets = load_dataset()\n","kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n","testloader = torch.utils.data.DataLoader(sets.test, batch_size=64, **kwargs)\n","\n","ConfusionMatrix().run(model, testloader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XP7nSRqhcucC","colab_type":"code","colab":{}},"source":["p =torch.arange(0, 5) % 3\n","print(p)\n","F.one_hot(p, 10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rHZ2XSodc0JH","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599797781532,"user_tz":-540,"elapsed":609,"user":{"displayName":"Yy Yy","photoUrl":"","userId":"02437050620110452344"}}},"source":["class Singleton(object):\n","  def __new__(cls, *args, **kargs):\n","    if not hasattr(cls, \"_instance\"):\n","      cls._instance = super(Singleton, cls).__new__(cls)\n","    return cls._instance\n","\n","class Myclass(Singleton):\n","  def __init__(self):\n","    pass\n","\n","  def store(self, f):\n","    self.f = f\n","\n","  def deco():\n","    def d(f):\n","      Myclass().store(f)\n","      def inner(**kwds):\n","        return f(**kwds)\n","      return inner\n","    return d\n","\n","@Myclass.deco()\n","def myfun():\n","  print(\"my function!\")"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"rLSOgdCzVDvv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1599797830189,"user_tz":-540,"elapsed":430,"user":{"displayName":"Yy Yy","photoUrl":"","userId":"02437050620110452344"}},"outputId":"7470ff28-4680-443c-9b77-98f903ca35f6"},"source":["c = Myclass()\n","c.f()"],"execution_count":14,"outputs":[{"output_type":"stream","text":["my function!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KWEjZo79VJKx","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}